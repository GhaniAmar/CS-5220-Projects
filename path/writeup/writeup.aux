\relax 
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Weak scaling results from the reference OpenMP implementation}}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Strong scaling results from the reference OpenMP implementation}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Strong scaling results from the naive MPI implementation. Note that running our code with eight threads is only a modest improvement over running it with two threads.}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Weak scaling results from the naive MPI implementation. Note that in the log-log plot, the curves are only displaced vertically from each other by a small amount: This indicates that the constant factor of improvement is not much from increasing the number of threads.}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The usual weak scaling experiments for the optimized MPI implementation. The log-log plot shows a good improvement in the constant factor with increased processor count, but, as expected, the asymptotic behavior remains the same.}}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces This strong scaling plot is a little misleading. There is a significant amount of overhead when we run the code with a few number of processors, so the 25 times speedup with eight processors isn't quite genuine.}}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Breakdown of time spent in computation and communication on a log-log plot for the ring MPI implementation.}}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A comparison of our timing experiments for the optimized MPI and OpenMP implementations. Note that the fastest MPI code runs roughly twice as fast as the OpenMP code for large graphs.}}{10}}
